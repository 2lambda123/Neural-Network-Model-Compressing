nohup: ignoring input
I0911 06:49:15.663512 33477 caffe.cpp:218] Using GPUs 0, 1, 2, 3
I0911 06:49:15.664332 33477 caffe.cpp:223] GPU 0: Tesla P40
I0911 06:49:15.664834 33477 caffe.cpp:223] GPU 1: Tesla P40
I0911 06:49:15.665313 33477 caffe.cpp:223] GPU 2: Tesla P40
I0911 06:49:15.665792 33477 caffe.cpp:223] GPU 3: Tesla P40
I0911 06:49:16.833868 33477 solver.cpp:44] Initializing solver from parameters: 
test_iter: 250
test_interval: 1000
base_lr: 0.01
display: 20
max_iter: 80000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 17000
snapshot: 10000
snapshot_prefix: "models_compression/alexnet/DNS_alexnet_conv"
solver_mode: GPU
device_id: 0
net: "models_compression/alexnet/train_val_DNS_conv.prototxt"
train_state {
  level: 0
  stage: ""
}
I0911 06:49:16.834363 33477 solver.cpp:87] Creating training net from net file: models_compression/alexnet/train_val_DNS_conv.prototxt
I0911 06:49:16.835711 33477 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0911 06:49:16.835764 33477 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0911 06:49:16.836266 33477 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/data/caffe-imagenet/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "CConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "CConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "CConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "CConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "CConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0911 06:49:16.836678 33477 layer_factory.hpp:77] Creating layer data
I0911 06:49:16.849647 33477 db_lmdb.cpp:35] Opened lmdb /home/data/caffe-imagenet/ilsvrc12_train_lmdb
I0911 06:49:16.867599 33477 net.cpp:84] Creating Layer data
I0911 06:49:16.867650 33477 net.cpp:380] data -> data
I0911 06:49:16.867712 33477 net.cpp:380] data -> label
I0911 06:49:16.867779 33477 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0911 06:49:16.899082 33477 data_layer.cpp:45] output data size: 256,3,227,227
I0911 06:49:17.323609 33477 net.cpp:122] Setting up data
I0911 06:49:17.323693 33477 net.cpp:129] Top shape: 256 3 227 227 (39574272)
I0911 06:49:17.323709 33477 net.cpp:129] Top shape: 256 (256)
I0911 06:49:17.323714 33477 net.cpp:137] Memory required for data: 158298112
I0911 06:49:17.323734 33477 layer_factory.hpp:77] Creating layer conv1
I0911 06:49:17.323949 33477 net.cpp:84] Creating Layer conv1
I0911 06:49:17.323993 33477 net.cpp:406] conv1 <- data
I0911 06:49:17.324018 33477 net.cpp:380] conv1 -> conv1
I0911 06:49:17.327803 33477 net.cpp:122] Setting up conv1
I0911 06:49:17.327824 33477 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0911 06:49:17.327831 33477 net.cpp:137] Memory required for data: 455667712
I0911 06:49:17.327865 33477 layer_factory.hpp:77] Creating layer relu1
I0911 06:49:17.327885 33477 net.cpp:84] Creating Layer relu1
I0911 06:49:17.327893 33477 net.cpp:406] relu1 <- conv1
I0911 06:49:17.327901 33477 net.cpp:367] relu1 -> conv1 (in-place)
I0911 06:49:18.539721 33477 net.cpp:122] Setting up relu1
I0911 06:49:18.539772 33477 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0911 06:49:18.539779 33477 net.cpp:137] Memory required for data: 753037312
I0911 06:49:18.539788 33477 layer_factory.hpp:77] Creating layer norm1
I0911 06:49:18.539813 33477 net.cpp:84] Creating Layer norm1
I0911 06:49:18.539820 33477 net.cpp:406] norm1 <- conv1
I0911 06:49:18.539831 33477 net.cpp:380] norm1 -> norm1
I0911 06:49:18.540997 33477 net.cpp:122] Setting up norm1
I0911 06:49:18.541015 33477 net.cpp:129] Top shape: 256 96 55 55 (74342400)
I0911 06:49:18.541021 33477 net.cpp:137] Memory required for data: 1050406912
I0911 06:49:18.541028 33477 layer_factory.hpp:77] Creating layer pool1
I0911 06:49:18.541043 33477 net.cpp:84] Creating Layer pool1
I0911 06:49:18.541049 33477 net.cpp:406] pool1 <- norm1
I0911 06:49:18.541056 33477 net.cpp:380] pool1 -> pool1
I0911 06:49:18.541115 33477 net.cpp:122] Setting up pool1
I0911 06:49:18.541126 33477 net.cpp:129] Top shape: 256 96 27 27 (17915904)
I0911 06:49:18.541129 33477 net.cpp:137] Memory required for data: 1122070528
I0911 06:49:18.541136 33477 layer_factory.hpp:77] Creating layer conv2
I0911 06:49:18.541149 33477 net.cpp:84] Creating Layer conv2
I0911 06:49:18.541154 33477 net.cpp:406] conv2 <- pool1
I0911 06:49:18.541162 33477 net.cpp:380] conv2 -> conv2
I0911 06:49:18.547983 33477 net.cpp:122] Setting up conv2
I0911 06:49:18.548002 33477 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I0911 06:49:18.548007 33477 net.cpp:137] Memory required for data: 1313173504
I0911 06:49:18.548024 33477 layer_factory.hpp:77] Creating layer relu2
I0911 06:49:18.548034 33477 net.cpp:84] Creating Layer relu2
I0911 06:49:18.548040 33477 net.cpp:406] relu2 <- conv2
I0911 06:49:18.548048 33477 net.cpp:367] relu2 -> conv2 (in-place)
I0911 06:49:18.548251 33477 net.cpp:122] Setting up relu2
I0911 06:49:18.548261 33477 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I0911 06:49:18.548266 33477 net.cpp:137] Memory required for data: 1504276480
I0911 06:49:18.548271 33477 layer_factory.hpp:77] Creating layer norm2
I0911 06:49:18.548283 33477 net.cpp:84] Creating Layer norm2
I0911 06:49:18.548288 33477 net.cpp:406] norm2 <- conv2
I0911 06:49:18.548295 33477 net.cpp:380] norm2 -> norm2
I0911 06:49:18.548517 33477 net.cpp:122] Setting up norm2
I0911 06:49:18.548529 33477 net.cpp:129] Top shape: 256 256 27 27 (47775744)
I0911 06:49:18.548534 33477 net.cpp:137] Memory required for data: 1695379456
I0911 06:49:18.548539 33477 layer_factory.hpp:77] Creating layer pool2
I0911 06:49:18.548552 33477 net.cpp:84] Creating Layer pool2
I0911 06:49:18.548557 33477 net.cpp:406] pool2 <- norm2
I0911 06:49:18.548563 33477 net.cpp:380] pool2 -> pool2
I0911 06:49:18.548602 33477 net.cpp:122] Setting up pool2
I0911 06:49:18.548609 33477 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0911 06:49:18.548614 33477 net.cpp:137] Memory required for data: 1739681792
I0911 06:49:18.548619 33477 layer_factory.hpp:77] Creating layer conv3
I0911 06:49:18.548630 33477 net.cpp:84] Creating Layer conv3
I0911 06:49:18.548635 33477 net.cpp:406] conv3 <- pool2
I0911 06:49:18.548642 33477 net.cpp:380] conv3 -> conv3
I0911 06:49:18.564141 33477 net.cpp:122] Setting up conv3
I0911 06:49:18.564162 33477 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0911 06:49:18.564167 33477 net.cpp:137] Memory required for data: 1806135296
I0911 06:49:18.564191 33477 layer_factory.hpp:77] Creating layer relu3
I0911 06:49:18.564200 33477 net.cpp:84] Creating Layer relu3
I0911 06:49:18.564234 33477 net.cpp:406] relu3 <- conv3
I0911 06:49:18.564241 33477 net.cpp:367] relu3 -> conv3 (in-place)
I0911 06:49:18.565192 33477 net.cpp:122] Setting up relu3
I0911 06:49:18.565209 33477 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0911 06:49:18.565214 33477 net.cpp:137] Memory required for data: 1872588800
I0911 06:49:18.565219 33477 layer_factory.hpp:77] Creating layer conv4
I0911 06:49:18.565237 33477 net.cpp:84] Creating Layer conv4
I0911 06:49:18.565243 33477 net.cpp:406] conv4 <- conv3
I0911 06:49:18.565250 33477 net.cpp:380] conv4 -> conv4
I0911 06:49:18.577190 33477 net.cpp:122] Setting up conv4
I0911 06:49:18.577209 33477 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0911 06:49:18.577214 33477 net.cpp:137] Memory required for data: 1939042304
I0911 06:49:18.577225 33477 layer_factory.hpp:77] Creating layer relu4
I0911 06:49:18.577235 33477 net.cpp:84] Creating Layer relu4
I0911 06:49:18.577240 33477 net.cpp:406] relu4 <- conv4
I0911 06:49:18.577250 33477 net.cpp:367] relu4 -> conv4 (in-place)
I0911 06:49:18.577476 33477 net.cpp:122] Setting up relu4
I0911 06:49:18.577487 33477 net.cpp:129] Top shape: 256 384 13 13 (16613376)
I0911 06:49:18.577492 33477 net.cpp:137] Memory required for data: 2005495808
I0911 06:49:18.577497 33477 layer_factory.hpp:77] Creating layer conv5
I0911 06:49:18.577509 33477 net.cpp:84] Creating Layer conv5
I0911 06:49:18.577514 33477 net.cpp:406] conv5 <- conv4
I0911 06:49:18.577524 33477 net.cpp:380] conv5 -> conv5
I0911 06:49:18.586165 33477 net.cpp:122] Setting up conv5
I0911 06:49:18.586184 33477 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0911 06:49:18.586189 33477 net.cpp:137] Memory required for data: 2049798144
I0911 06:49:18.586205 33477 layer_factory.hpp:77] Creating layer relu5
I0911 06:49:18.586215 33477 net.cpp:84] Creating Layer relu5
I0911 06:49:18.586220 33477 net.cpp:406] relu5 <- conv5
I0911 06:49:18.586227 33477 net.cpp:367] relu5 -> conv5 (in-place)
I0911 06:49:18.586458 33477 net.cpp:122] Setting up relu5
I0911 06:49:18.586470 33477 net.cpp:129] Top shape: 256 256 13 13 (11075584)
I0911 06:49:18.586475 33477 net.cpp:137] Memory required for data: 2094100480
I0911 06:49:18.586482 33477 layer_factory.hpp:77] Creating layer pool5
I0911 06:49:18.586493 33477 net.cpp:84] Creating Layer pool5
I0911 06:49:18.586498 33477 net.cpp:406] pool5 <- conv5
I0911 06:49:18.586504 33477 net.cpp:380] pool5 -> pool5
I0911 06:49:18.586555 33477 net.cpp:122] Setting up pool5
I0911 06:49:18.586563 33477 net.cpp:129] Top shape: 256 256 6 6 (2359296)
I0911 06:49:18.586567 33477 net.cpp:137] Memory required for data: 2103537664
I0911 06:49:18.586572 33477 layer_factory.hpp:77] Creating layer fc6
I0911 06:49:18.586588 33477 net.cpp:84] Creating Layer fc6
I0911 06:49:18.586593 33477 net.cpp:406] fc6 <- pool5
I0911 06:49:18.586601 33477 net.cpp:380] fc6 -> fc6
I0911 06:49:19.094825 33477 net.cpp:122] Setting up fc6
I0911 06:49:19.094872 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.094878 33477 net.cpp:137] Memory required for data: 2107731968
I0911 06:49:19.094892 33477 layer_factory.hpp:77] Creating layer relu6
I0911 06:49:19.094905 33477 net.cpp:84] Creating Layer relu6
I0911 06:49:19.094911 33477 net.cpp:406] relu6 <- fc6
I0911 06:49:19.094923 33477 net.cpp:367] relu6 -> fc6 (in-place)
I0911 06:49:19.095232 33477 net.cpp:122] Setting up relu6
I0911 06:49:19.095242 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.095247 33477 net.cpp:137] Memory required for data: 2111926272
I0911 06:49:19.095252 33477 layer_factory.hpp:77] Creating layer drop6
I0911 06:49:19.095270 33477 net.cpp:84] Creating Layer drop6
I0911 06:49:19.095276 33477 net.cpp:406] drop6 <- fc6
I0911 06:49:19.095284 33477 net.cpp:367] drop6 -> fc6 (in-place)
I0911 06:49:19.095319 33477 net.cpp:122] Setting up drop6
I0911 06:49:19.095329 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.095332 33477 net.cpp:137] Memory required for data: 2116120576
I0911 06:49:19.095351 33477 layer_factory.hpp:77] Creating layer fc7
I0911 06:49:19.095398 33477 net.cpp:84] Creating Layer fc7
I0911 06:49:19.095404 33477 net.cpp:406] fc7 <- fc6
I0911 06:49:19.095414 33477 net.cpp:380] fc7 -> fc7
I0911 06:49:19.321213 33477 net.cpp:122] Setting up fc7
I0911 06:49:19.321261 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.321267 33477 net.cpp:137] Memory required for data: 2120314880
I0911 06:49:19.321280 33477 layer_factory.hpp:77] Creating layer relu7
I0911 06:49:19.321293 33477 net.cpp:84] Creating Layer relu7
I0911 06:49:19.321300 33477 net.cpp:406] relu7 <- fc7
I0911 06:49:19.321311 33477 net.cpp:367] relu7 -> fc7 (in-place)
I0911 06:49:19.322567 33477 net.cpp:122] Setting up relu7
I0911 06:49:19.322584 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.322590 33477 net.cpp:137] Memory required for data: 2124509184
I0911 06:49:19.322595 33477 layer_factory.hpp:77] Creating layer drop7
I0911 06:49:19.322608 33477 net.cpp:84] Creating Layer drop7
I0911 06:49:19.322613 33477 net.cpp:406] drop7 <- fc7
I0911 06:49:19.322620 33477 net.cpp:367] drop7 -> fc7 (in-place)
I0911 06:49:19.322651 33477 net.cpp:122] Setting up drop7
I0911 06:49:19.322659 33477 net.cpp:129] Top shape: 256 4096 (1048576)
I0911 06:49:19.322662 33477 net.cpp:137] Memory required for data: 2128703488
I0911 06:49:19.322667 33477 layer_factory.hpp:77] Creating layer fc8
I0911 06:49:19.322679 33477 net.cpp:84] Creating Layer fc8
I0911 06:49:19.322685 33477 net.cpp:406] fc8 <- fc7
I0911 06:49:19.322691 33477 net.cpp:380] fc8 -> fc8
I0911 06:49:19.378451 33477 net.cpp:122] Setting up fc8
I0911 06:49:19.378470 33477 net.cpp:129] Top shape: 256 1000 (256000)
I0911 06:49:19.378474 33477 net.cpp:137] Memory required for data: 2129727488
I0911 06:49:19.378484 33477 layer_factory.hpp:77] Creating layer loss
I0911 06:49:19.378504 33477 net.cpp:84] Creating Layer loss
I0911 06:49:19.378509 33477 net.cpp:406] loss <- fc8
I0911 06:49:19.378515 33477 net.cpp:406] loss <- label
I0911 06:49:19.378525 33477 net.cpp:380] loss -> loss
I0911 06:49:19.378588 33477 layer_factory.hpp:77] Creating layer loss
I0911 06:49:19.380465 33477 net.cpp:122] Setting up loss
I0911 06:49:19.380486 33477 net.cpp:129] Top shape: (1)
I0911 06:49:19.380491 33477 net.cpp:132]     with loss weight 1
I0911 06:49:19.380523 33477 net.cpp:137] Memory required for data: 2129727492
I0911 06:49:19.380528 33477 net.cpp:198] loss needs backward computation.
I0911 06:49:19.380534 33477 net.cpp:198] fc8 needs backward computation.
I0911 06:49:19.380539 33477 net.cpp:198] drop7 needs backward computation.
I0911 06:49:19.380543 33477 net.cpp:198] relu7 needs backward computation.
I0911 06:49:19.380548 33477 net.cpp:198] fc7 needs backward computation.
I0911 06:49:19.380553 33477 net.cpp:198] drop6 needs backward computation.
I0911 06:49:19.380556 33477 net.cpp:198] relu6 needs backward computation.
I0911 06:49:19.380561 33477 net.cpp:198] fc6 needs backward computation.
I0911 06:49:19.380566 33477 net.cpp:198] pool5 needs backward computation.
I0911 06:49:19.380570 33477 net.cpp:198] relu5 needs backward computation.
I0911 06:49:19.380575 33477 net.cpp:198] conv5 needs backward computation.
I0911 06:49:19.380580 33477 net.cpp:198] relu4 needs backward computation.
I0911 06:49:19.380584 33477 net.cpp:198] conv4 needs backward computation.
I0911 06:49:19.380589 33477 net.cpp:198] relu3 needs backward computation.
I0911 06:49:19.380592 33477 net.cpp:198] conv3 needs backward computation.
I0911 06:49:19.380597 33477 net.cpp:198] pool2 needs backward computation.
I0911 06:49:19.380602 33477 net.cpp:198] norm2 needs backward computation.
I0911 06:49:19.380606 33477 net.cpp:198] relu2 needs backward computation.
I0911 06:49:19.380611 33477 net.cpp:198] conv2 needs backward computation.
I0911 06:49:19.380615 33477 net.cpp:198] pool1 needs backward computation.
I0911 06:49:19.380620 33477 net.cpp:198] norm1 needs backward computation.
I0911 06:49:19.380625 33477 net.cpp:198] relu1 needs backward computation.
I0911 06:49:19.380628 33477 net.cpp:198] conv1 needs backward computation.
I0911 06:49:19.380642 33477 net.cpp:200] data does not need backward computation.
I0911 06:49:19.380674 33477 net.cpp:242] This network produces output loss
I0911 06:49:19.380697 33477 net.cpp:255] Network initialization done.
I0911 06:49:19.381598 33477 solver.cpp:172] Creating test net (#0) specified by net file: models_compression/alexnet/train_val_DNS_conv.prototxt
I0911 06:49:19.381654 33477 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0911 06:49:19.382009 33477 net.cpp:51] Initializing net from parameters: 
name: "AlexNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 227
    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/home/data/caffe-imagenet/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "CConvolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "CConvolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "CConvolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "CConvolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "CConvolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
  cconvolution_param {
    gamma: 1.25e-05
    power: 1
    iter_stop: 450000
    c_rate: 2
    weight_mask_filler {
      type: "constant"
      value: 1
    }
    bias_mask_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I0911 06:49:19.382179 33477 layer_factory.hpp:77] Creating layer data
I0911 06:49:19.406435 33477 db_lmdb.cpp:35] Opened lmdb /home/data/caffe-imagenet/ilsvrc12_val_lmdb
I0911 06:49:19.419761 33477 net.cpp:84] Creating Layer data
I0911 06:49:19.419798 33477 net.cpp:380] data -> data
I0911 06:49:19.419821 33477 net.cpp:380] data -> label
I0911 06:49:19.419839 33477 data_transformer.cpp:25] Loading mean file from: data/ilsvrc12/imagenet_mean.binaryproto
I0911 06:49:19.448812 33477 data_layer.cpp:45] output data size: 50,3,227,227
I0911 06:49:19.546375 33477 net.cpp:122] Setting up data
I0911 06:49:19.546437 33477 net.cpp:129] Top shape: 50 3 227 227 (7729350)
I0911 06:49:19.546448 33477 net.cpp:129] Top shape: 50 (50)
I0911 06:49:19.546453 33477 net.cpp:137] Memory required for data: 30917600
I0911 06:49:19.546464 33477 layer_factory.hpp:77] Creating layer label_data_1_split
I0911 06:49:19.555479 33477 net.cpp:84] Creating Layer label_data_1_split
I0911 06:49:19.555492 33477 net.cpp:406] label_data_1_split <- label
I0911 06:49:19.555502 33477 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0911 06:49:19.555517 33477 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0911 06:49:19.555598 33477 net.cpp:122] Setting up label_data_1_split
I0911 06:49:19.555606 33477 net.cpp:129] Top shape: 50 (50)
I0911 06:49:19.555613 33477 net.cpp:129] Top shape: 50 (50)
I0911 06:49:19.555627 33477 net.cpp:137] Memory required for data: 30918000
I0911 06:49:19.555661 33477 layer_factory.hpp:77] Creating layer conv1
I0911 06:49:19.555680 33477 net.cpp:84] Creating Layer conv1
I0911 06:49:19.555685 33477 net.cpp:406] conv1 <- data
I0911 06:49:19.555693 33477 net.cpp:380] conv1 -> conv1
I0911 06:49:19.556584 33477 net.cpp:122] Setting up conv1
I0911 06:49:19.556597 33477 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0911 06:49:19.556602 33477 net.cpp:137] Memory required for data: 88998000
I0911 06:49:19.556620 33477 layer_factory.hpp:77] Creating layer relu1
I0911 06:49:19.556630 33477 net.cpp:84] Creating Layer relu1
I0911 06:49:19.556637 33477 net.cpp:406] relu1 <- conv1
I0911 06:49:19.556643 33477 net.cpp:367] relu1 -> conv1 (in-place)
I0911 06:49:19.556903 33477 net.cpp:122] Setting up relu1
I0911 06:49:19.556915 33477 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0911 06:49:19.556919 33477 net.cpp:137] Memory required for data: 147078000
I0911 06:49:19.556923 33477 layer_factory.hpp:77] Creating layer norm1
I0911 06:49:19.556936 33477 net.cpp:84] Creating Layer norm1
I0911 06:49:19.556941 33477 net.cpp:406] norm1 <- conv1
I0911 06:49:19.556947 33477 net.cpp:380] norm1 -> norm1
I0911 06:49:19.557189 33477 net.cpp:122] Setting up norm1
I0911 06:49:19.557199 33477 net.cpp:129] Top shape: 50 96 55 55 (14520000)
I0911 06:49:19.557204 33477 net.cpp:137] Memory required for data: 205158000
I0911 06:49:19.557209 33477 layer_factory.hpp:77] Creating layer pool1
I0911 06:49:19.557219 33477 net.cpp:84] Creating Layer pool1
I0911 06:49:19.557222 33477 net.cpp:406] pool1 <- norm1
I0911 06:49:19.557229 33477 net.cpp:380] pool1 -> pool1
I0911 06:49:19.557274 33477 net.cpp:122] Setting up pool1
I0911 06:49:19.557281 33477 net.cpp:129] Top shape: 50 96 27 27 (3499200)
I0911 06:49:19.557286 33477 net.cpp:137] Memory required for data: 219154800
I0911 06:49:19.557289 33477 layer_factory.hpp:77] Creating layer conv2
I0911 06:49:19.557301 33477 net.cpp:84] Creating Layer conv2
I0911 06:49:19.557305 33477 net.cpp:406] conv2 <- pool1
I0911 06:49:19.557313 33477 net.cpp:380] conv2 -> conv2
I0911 06:49:19.564241 33477 net.cpp:122] Setting up conv2
I0911 06:49:19.564261 33477 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I0911 06:49:19.564266 33477 net.cpp:137] Memory required for data: 256479600
I0911 06:49:19.564281 33477 layer_factory.hpp:77] Creating layer relu2
I0911 06:49:19.564291 33477 net.cpp:84] Creating Layer relu2
I0911 06:49:19.564296 33477 net.cpp:406] relu2 <- conv2
I0911 06:49:19.564304 33477 net.cpp:367] relu2 -> conv2 (in-place)
I0911 06:49:19.565327 33477 net.cpp:122] Setting up relu2
I0911 06:49:19.565346 33477 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I0911 06:49:19.565351 33477 net.cpp:137] Memory required for data: 293804400
I0911 06:49:19.565356 33477 layer_factory.hpp:77] Creating layer norm2
I0911 06:49:19.565367 33477 net.cpp:84] Creating Layer norm2
I0911 06:49:19.565373 33477 net.cpp:406] norm2 <- conv2
I0911 06:49:19.565388 33477 net.cpp:380] norm2 -> norm2
I0911 06:49:19.565627 33477 net.cpp:122] Setting up norm2
I0911 06:49:19.565639 33477 net.cpp:129] Top shape: 50 256 27 27 (9331200)
I0911 06:49:19.565644 33477 net.cpp:137] Memory required for data: 331129200
I0911 06:49:19.565649 33477 layer_factory.hpp:77] Creating layer pool2
I0911 06:49:19.565657 33477 net.cpp:84] Creating Layer pool2
I0911 06:49:19.565662 33477 net.cpp:406] pool2 <- norm2
I0911 06:49:19.565670 33477 net.cpp:380] pool2 -> pool2
I0911 06:49:19.565714 33477 net.cpp:122] Setting up pool2
I0911 06:49:19.565722 33477 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0911 06:49:19.565726 33477 net.cpp:137] Memory required for data: 339782000
I0911 06:49:19.565732 33477 layer_factory.hpp:77] Creating layer conv3
I0911 06:49:19.565742 33477 net.cpp:84] Creating Layer conv3
I0911 06:49:19.565747 33477 net.cpp:406] conv3 <- pool2
I0911 06:49:19.565757 33477 net.cpp:380] conv3 -> conv3
I0911 06:49:19.581488 33477 net.cpp:122] Setting up conv3
I0911 06:49:19.581507 33477 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0911 06:49:19.581518 33477 net.cpp:137] Memory required for data: 352761200
I0911 06:49:19.581547 33477 layer_factory.hpp:77] Creating layer relu3
I0911 06:49:19.581557 33477 net.cpp:84] Creating Layer relu3
I0911 06:49:19.581562 33477 net.cpp:406] relu3 <- conv3
I0911 06:49:19.581569 33477 net.cpp:367] relu3 -> conv3 (in-place)
I0911 06:49:19.581784 33477 net.cpp:122] Setting up relu3
I0911 06:49:19.581794 33477 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0911 06:49:19.581799 33477 net.cpp:137] Memory required for data: 365740400
I0911 06:49:19.581804 33477 layer_factory.hpp:77] Creating layer conv4
I0911 06:49:19.581815 33477 net.cpp:84] Creating Layer conv4
I0911 06:49:19.581820 33477 net.cpp:406] conv4 <- conv3
I0911 06:49:19.581830 33477 net.cpp:380] conv4 -> conv4
I0911 06:49:19.593870 33477 net.cpp:122] Setting up conv4
I0911 06:49:19.593890 33477 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0911 06:49:19.593895 33477 net.cpp:137] Memory required for data: 378719600
I0911 06:49:19.593907 33477 layer_factory.hpp:77] Creating layer relu4
I0911 06:49:19.593916 33477 net.cpp:84] Creating Layer relu4
I0911 06:49:19.593921 33477 net.cpp:406] relu4 <- conv4
I0911 06:49:19.593930 33477 net.cpp:367] relu4 -> conv4 (in-place)
I0911 06:49:19.594935 33477 net.cpp:122] Setting up relu4
I0911 06:49:19.594952 33477 net.cpp:129] Top shape: 50 384 13 13 (3244800)
I0911 06:49:19.594957 33477 net.cpp:137] Memory required for data: 391698800
I0911 06:49:19.594964 33477 layer_factory.hpp:77] Creating layer conv5
I0911 06:49:19.594975 33477 net.cpp:84] Creating Layer conv5
I0911 06:49:19.594981 33477 net.cpp:406] conv5 <- conv4
I0911 06:49:19.594991 33477 net.cpp:380] conv5 -> conv5
I0911 06:49:19.603708 33477 net.cpp:122] Setting up conv5
I0911 06:49:19.603729 33477 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0911 06:49:19.603732 33477 net.cpp:137] Memory required for data: 400351600
I0911 06:49:19.603749 33477 layer_factory.hpp:77] Creating layer relu5
I0911 06:49:19.603760 33477 net.cpp:84] Creating Layer relu5
I0911 06:49:19.603765 33477 net.cpp:406] relu5 <- conv5
I0911 06:49:19.603772 33477 net.cpp:367] relu5 -> conv5 (in-place)
I0911 06:49:19.603988 33477 net.cpp:122] Setting up relu5
I0911 06:49:19.603998 33477 net.cpp:129] Top shape: 50 256 13 13 (2163200)
I0911 06:49:19.604003 33477 net.cpp:137] Memory required for data: 409004400
I0911 06:49:19.604008 33477 layer_factory.hpp:77] Creating layer pool5
I0911 06:49:19.604018 33477 net.cpp:84] Creating Layer pool5
I0911 06:49:19.604023 33477 net.cpp:406] pool5 <- conv5
I0911 06:49:19.604032 33477 net.cpp:380] pool5 -> pool5
I0911 06:49:19.604081 33477 net.cpp:122] Setting up pool5
I0911 06:49:19.604089 33477 net.cpp:129] Top shape: 50 256 6 6 (460800)
I0911 06:49:19.604094 33477 net.cpp:137] Memory required for data: 410847600
I0911 06:49:19.604097 33477 layer_factory.hpp:77] Creating layer fc6
I0911 06:49:19.604107 33477 net.cpp:84] Creating Layer fc6
I0911 06:49:19.604112 33477 net.cpp:406] fc6 <- pool5
I0911 06:49:19.604120 33477 net.cpp:380] fc6 -> fc6
I0911 06:49:20.111114 33477 net.cpp:122] Setting up fc6
I0911 06:49:20.111162 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.111168 33477 net.cpp:137] Memory required for data: 411666800
I0911 06:49:20.111186 33477 layer_factory.hpp:77] Creating layer relu6
I0911 06:49:20.111207 33477 net.cpp:84] Creating Layer relu6
I0911 06:49:20.111215 33477 net.cpp:406] relu6 <- fc6
I0911 06:49:20.111224 33477 net.cpp:367] relu6 -> fc6 (in-place)
I0911 06:49:20.111543 33477 net.cpp:122] Setting up relu6
I0911 06:49:20.111554 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.111559 33477 net.cpp:137] Memory required for data: 412486000
I0911 06:49:20.111564 33477 layer_factory.hpp:77] Creating layer drop6
I0911 06:49:20.111577 33477 net.cpp:84] Creating Layer drop6
I0911 06:49:20.111582 33477 net.cpp:406] drop6 <- fc6
I0911 06:49:20.111588 33477 net.cpp:367] drop6 -> fc6 (in-place)
I0911 06:49:20.111632 33477 net.cpp:122] Setting up drop6
I0911 06:49:20.111639 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.111657 33477 net.cpp:137] Memory required for data: 413305200
I0911 06:49:20.111693 33477 layer_factory.hpp:77] Creating layer fc7
I0911 06:49:20.111706 33477 net.cpp:84] Creating Layer fc7
I0911 06:49:20.111711 33477 net.cpp:406] fc7 <- fc6
I0911 06:49:20.111718 33477 net.cpp:380] fc7 -> fc7
I0911 06:49:20.337299 33477 net.cpp:122] Setting up fc7
I0911 06:49:20.337350 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.337357 33477 net.cpp:137] Memory required for data: 414124400
I0911 06:49:20.337370 33477 layer_factory.hpp:77] Creating layer relu7
I0911 06:49:20.337389 33477 net.cpp:84] Creating Layer relu7
I0911 06:49:20.337397 33477 net.cpp:406] relu7 <- fc7
I0911 06:49:20.337406 33477 net.cpp:367] relu7 -> fc7 (in-place)
I0911 06:49:20.337693 33477 net.cpp:122] Setting up relu7
I0911 06:49:20.337704 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.337709 33477 net.cpp:137] Memory required for data: 414943600
I0911 06:49:20.337713 33477 layer_factory.hpp:77] Creating layer drop7
I0911 06:49:20.337728 33477 net.cpp:84] Creating Layer drop7
I0911 06:49:20.337733 33477 net.cpp:406] drop7 <- fc7
I0911 06:49:20.337740 33477 net.cpp:367] drop7 -> fc7 (in-place)
I0911 06:49:20.337776 33477 net.cpp:122] Setting up drop7
I0911 06:49:20.337783 33477 net.cpp:129] Top shape: 50 4096 (204800)
I0911 06:49:20.337788 33477 net.cpp:137] Memory required for data: 415762800
I0911 06:49:20.337792 33477 layer_factory.hpp:77] Creating layer fc8
I0911 06:49:20.337805 33477 net.cpp:84] Creating Layer fc8
I0911 06:49:20.337810 33477 net.cpp:406] fc8 <- fc7
I0911 06:49:20.337818 33477 net.cpp:380] fc8 -> fc8
I0911 06:49:20.393702 33477 net.cpp:122] Setting up fc8
I0911 06:49:20.393721 33477 net.cpp:129] Top shape: 50 1000 (50000)
I0911 06:49:20.393726 33477 net.cpp:137] Memory required for data: 415962800
I0911 06:49:20.393735 33477 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I0911 06:49:20.393748 33477 net.cpp:84] Creating Layer fc8_fc8_0_split
I0911 06:49:20.393754 33477 net.cpp:406] fc8_fc8_0_split <- fc8
I0911 06:49:20.393762 33477 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_0
I0911 06:49:20.393771 33477 net.cpp:380] fc8_fc8_0_split -> fc8_fc8_0_split_1
I0911 06:49:20.393818 33477 net.cpp:122] Setting up fc8_fc8_0_split
I0911 06:49:20.393827 33477 net.cpp:129] Top shape: 50 1000 (50000)
I0911 06:49:20.393832 33477 net.cpp:129] Top shape: 50 1000 (50000)
I0911 06:49:20.393837 33477 net.cpp:137] Memory required for data: 416362800
I0911 06:49:20.393841 33477 layer_factory.hpp:77] Creating layer accuracy
I0911 06:49:20.393856 33477 net.cpp:84] Creating Layer accuracy
I0911 06:49:20.393860 33477 net.cpp:406] accuracy <- fc8_fc8_0_split_0
I0911 06:49:20.393867 33477 net.cpp:406] accuracy <- label_data_1_split_0
I0911 06:49:20.393874 33477 net.cpp:380] accuracy -> accuracy
I0911 06:49:20.393887 33477 net.cpp:122] Setting up accuracy
I0911 06:49:20.393893 33477 net.cpp:129] Top shape: (1)
I0911 06:49:20.393896 33477 net.cpp:137] Memory required for data: 416362804
I0911 06:49:20.393901 33477 layer_factory.hpp:77] Creating layer loss
I0911 06:49:20.393909 33477 net.cpp:84] Creating Layer loss
I0911 06:49:20.393914 33477 net.cpp:406] loss <- fc8_fc8_0_split_1
I0911 06:49:20.393920 33477 net.cpp:406] loss <- label_data_1_split_1
I0911 06:49:20.393928 33477 net.cpp:380] loss -> loss
I0911 06:49:20.393936 33477 layer_factory.hpp:77] Creating layer loss
I0911 06:49:20.395174 33477 net.cpp:122] Setting up loss
I0911 06:49:20.395192 33477 net.cpp:129] Top shape: (1)
I0911 06:49:20.395197 33477 net.cpp:132]     with loss weight 1
I0911 06:49:20.395212 33477 net.cpp:137] Memory required for data: 416362808
I0911 06:49:20.395217 33477 net.cpp:198] loss needs backward computation.
I0911 06:49:20.395223 33477 net.cpp:200] accuracy does not need backward computation.
I0911 06:49:20.395229 33477 net.cpp:198] fc8_fc8_0_split needs backward computation.
I0911 06:49:20.395234 33477 net.cpp:198] fc8 needs backward computation.
I0911 06:49:20.395239 33477 net.cpp:198] drop7 needs backward computation.
I0911 06:49:20.395254 33477 net.cpp:198] relu7 needs backward computation.
I0911 06:49:20.395290 33477 net.cpp:198] fc7 needs backward computation.
I0911 06:49:20.395297 33477 net.cpp:198] drop6 needs backward computation.
I0911 06:49:20.395300 33477 net.cpp:198] relu6 needs backward computation.
I0911 06:49:20.395304 33477 net.cpp:198] fc6 needs backward computation.
I0911 06:49:20.395308 33477 net.cpp:198] pool5 needs backward computation.
I0911 06:49:20.395313 33477 net.cpp:198] relu5 needs backward computation.
I0911 06:49:20.395318 33477 net.cpp:198] conv5 needs backward computation.
I0911 06:49:20.395323 33477 net.cpp:198] relu4 needs backward computation.
I0911 06:49:20.395328 33477 net.cpp:198] conv4 needs backward computation.
I0911 06:49:20.395331 33477 net.cpp:198] relu3 needs backward computation.
I0911 06:49:20.395336 33477 net.cpp:198] conv3 needs backward computation.
I0911 06:49:20.395341 33477 net.cpp:198] pool2 needs backward computation.
I0911 06:49:20.395345 33477 net.cpp:198] norm2 needs backward computation.
I0911 06:49:20.395350 33477 net.cpp:198] relu2 needs backward computation.
I0911 06:49:20.395355 33477 net.cpp:198] conv2 needs backward computation.
I0911 06:49:20.395360 33477 net.cpp:198] pool1 needs backward computation.
I0911 06:49:20.395366 33477 net.cpp:198] norm1 needs backward computation.
I0911 06:49:20.395370 33477 net.cpp:198] relu1 needs backward computation.
I0911 06:49:20.395375 33477 net.cpp:198] conv1 needs backward computation.
I0911 06:49:20.396037 33477 net.cpp:200] label_data_1_split does not need backward computation.
I0911 06:49:20.396055 33477 net.cpp:200] data does not need backward computation.
I0911 06:49:20.396060 33477 net.cpp:242] This network produces output accuracy
I0911 06:49:20.396070 33477 net.cpp:242] This network produces output loss
I0911 06:49:20.396095 33477 net.cpp:255] Network initialization done.
I0911 06:49:20.396237 33477 solver.cpp:56] Solver scaffolding done.
I0911 06:49:20.397483 33477 caffe.cpp:155] Finetuning from models_compression/alexnet/DNS_alexnet_train_iter_112500.caffemodel
I0911 06:49:21.258937 33477 caffe.cpp:248] Starting Optimization
F0911 06:49:21.260998 33477 parallel.cpp:83] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
    @     0x7f062a81e84d  google::LogMessage::Fail()
    @     0x7f062a82061c  google::LogMessage::SendToLog()
    @     0x7f062a81e43c  google::LogMessage::Flush()
    @     0x7f062a820f2e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f06319a6cb8  caffe::GPUParams<>::GPUParams()
    @     0x7f06319a6f18  caffe::NCCL<>::NCCL()
    @           0x40adb8  train()
    @           0x4082ec  main
    @     0x7f061f2b6af5  __libc_start_main
    @           0x408bf5  (unknown)
models_compression/alexnet/train_alexnet_DNS_conv.sh: line 7: 33477 Aborted                 ./build/tools/caffe train --solver=models_compression/alexnet/alexnet_solver_DNS_conv.prototxt --weights=models_compression/alexnet/DNS_alexnet_train_iter_112500.caffemodel --gpu=all $@
